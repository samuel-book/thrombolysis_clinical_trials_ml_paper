{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost to predict discharge disability: Do we have enough data?\n",
    "Learning curves: How much data do we need? Do we have enough?\n",
    "\n",
    "## Plain English summary\n",
    "One simple method to see whether we have enough data is to examine how accuracy increases with training set size. Does accuracy plateau before we hit the limit to how much data we have? In that case, getting more data will not help the model significantly. Is accuracy still increasing as we reach the limit to our data size? If so we would likely benefit from more data, if we can get it.\n",
    "\n",
    "We will wrap the model in a loop to increase the training set data size (taking a different random training/test split each time, and keeping the test set the same size). We will have an inner loop to perform 10 replicates at each sample size (to reduce the variation in our results)\n",
    "\n",
    "## Model and data\n",
    "Model: XGBoost classifier (multiclass classification) [from notebook 010]\\\n",
    "Target feature: Discharge disability\\\n",
    "Input features: All the relevant features in SSNAP\\\n",
    "Number of instances: different sizes of training data\\\n",
    "Kfold split: First kfold split\n",
    "\n",
    "We will go through the following steps:\n",
    "\n",
    "* Download and save pre-processed data\n",
    "* Split data into features (X) and label (y)\n",
    "* Split data into training and test sets (we will test on data that has not been used to fit the model)\n",
    "* Standardise data\n",
    "* Loop with increasing training set size:\n",
    "    * Loop through 10 replicates\n",
    "        * Fit a logistic regression model (from sklearn)\n",
    "        * Predict survival of the test set\n",
    "* Plot the relationship between training set size and accuracy\n",
    "\n",
    "## Aims\n",
    "Do we have enough data?\n",
    "\n",
    "## Observations\n",
    "It appears that we need all, if not more, data to obtain the best accuracy.\n",
    "\n",
    "## Further work\n",
    "Look at feature selection, see notebook 020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules\n",
    "\n",
    "A standard Anaconda install of Python (https://www.anaconda.com/distribution/) contains all the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import machine learning methods\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Turn warnings off to keep notebook tidy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the time duration to run notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_binary_thresholds = [0, 1, 2, 3, 4, 5]\n",
    "n_binary_models = len(list_binary_thresholds)\n",
    "surrogate_time_for_no_thrombolysis = 9999\n",
    "target_feature = 'discharge_disability'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up paths and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    '''Singleton object for storing paths to data and database.'''\n",
    "    image_save_path: str = './saved_images'\n",
    "    model_save_path: str = './saved_models'\n",
    "    data_save_path: str = './saved_data'\n",
    "    data_read_path: str = '../data_processing/output'\n",
    "    model_text: str = f'xgb_all_features_learning_curve_binary'\n",
    "    notebook: str = '015_'\n",
    "\n",
    "paths = Paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(paths.data_read_path, '02_reformatted_data_ml_remove_mt.csv')\n",
    "data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features need to be removed from the dataset (those that are duplicates). Define a function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df, cols):\n",
    "    \"\"\"\n",
    "    For the dataframe, remove the columns 'cols' if they are present\n",
    "    \n",
    "    Args:\n",
    "        df [dataframe]: The feature values per patient\n",
    "        cols [list]: The features to remove if present\n",
    "\n",
    "    Return:\n",
    "        df [dataframe]: The feature values per patient without specified columns\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        if col in df.columns: df.drop([col],axis=1,inplace=True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_remove = ['id','stroke_team_id']\n",
    "data = drop_columns(data, cols_remove)\n",
    "feature_names = list(data)\n",
    "# number of dependant features\n",
    "n_features = len(feature_names) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 target categories\n"
     ]
    }
   ],
   "source": [
    "class_names = data[target_feature].unique()\n",
    "class_names = np.sort(class_names)\n",
    "n_classes = len(class_names)\n",
    "print(f'There are {n_classes} target categories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into X (features) and y (labels)\n",
    "\n",
    "We will separate out our features (the data we use to make a prediction) from our label (what we are truing to predict).\n",
    "By convention our features are called `X` (usually upper case to denote multiple features), and the label (survive or not) `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X and y\n",
    "X = data.drop(target_feature, axis=1)\n",
    "y = data[target_feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot the categorical features\n",
    "\n",
    "Convert some categorical features to one hot encoded features.\n",
    "\n",
    "Define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feature_to_one_hot(df, feature_name, prefix):\n",
    "    \"\"\"\n",
    "    Converts a categorical feature into a one hot encoded feature\n",
    "    \n",
    "    Args:\n",
    "        df [dataframe]: training or test dataset\n",
    "        feature_name [str]: feature to convert to one hot encoding\n",
    "        prefix [str]: string to use on new feature\n",
    "\n",
    "    Return:\n",
    "        df [dataframe]: One hot encoded representation of the feature\n",
    "    \"\"\"\n",
    "\n",
    "    # One hot encode a feature\n",
    "    df_feature = pd.get_dummies(\n",
    "        df[feature_name], prefix = prefix)\n",
    "    df = pd.concat([df, df_feature], axis=1)\n",
    "    df.drop(feature_name, axis=1, inplace=True)\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up two lists for the one hot encoding. \n",
    "\n",
    "A list of the feature names that are categorical and to be converted using one hot encoding.\n",
    "A list of the prefixes to use for these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_one_hot = [\"stroke_team\", \"weekday\"]\n",
    "list_prefix = [\"team\", \"weekday\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each feature in the list, fconvert to one hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, prefix in zip(features_to_one_hot, list_prefix):\n",
    "    X = convert_feature_to_one_hot(X, feature, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature names with one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ohe = list(X)\n",
    "n_features_ohe = len(features_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 54 original features (before one-hot encoding)\n",
      "There are 177 features (after one-hot encoding)\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {n_features} original features \"\n",
    "      f\"(before one-hot encoding)\")\n",
    "print(f\"There are {n_features_ohe} features (after one-hot encoding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the maximum training set size we can use \n",
    "We will use 25% of data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max training size: 126260\n"
     ]
    }
   ],
   "source": [
    "test_fraction = 0.25\n",
    "data_rows = X.shape[0]\n",
    "max_training_size = int(data_rows * (1 - test_fraction))\n",
    "print('Max training size: {}'.format(max_training_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost models\n",
    "Loop through increasing training set sizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_diff_training_set_sizes(n_classes, X, y):\n",
    "\n",
    "    # Set up list to collect results\n",
    "    results_training_size = []\n",
    "    results_roc_auc = []\n",
    "\n",
    "    for train_size in range(1000, max_training_size, 1000):\n",
    "        replicate_roc_auc = []\n",
    "        for replicate in range(10):\n",
    "            y_train = np.array([])\n",
    "            # Keep spliting until get one of each category in the training set\n",
    "            while len(np.unique(y_train))!=n_classes:\n",
    "                # Split data into training and test\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y, test_size = test_fraction)\n",
    "\n",
    "                # Reduce training set size (use np random choice for random index values)\n",
    "                selection_index = np.random.choice(\n",
    "                    max_training_size, train_size, replace=False)\n",
    "                X_train = X_train.iloc[selection_index]\n",
    "                y_train = y_train.iloc[selection_index]\n",
    "\n",
    "            # Define model\n",
    "            model = XGBClassifier(verbosity=0, seed=42)\n",
    "            # Fit model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Get target categories from model\n",
    "            classes = model.classes_\n",
    "\n",
    "            # Get predicted probabilities\n",
    "            y_probs = model.predict_proba(X_test)\n",
    "\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, y_probs[:,1])\n",
    "            rocauc = auc(fpr, tpr)\n",
    "\n",
    "            # Record results\n",
    "            replicate_roc_auc.append(rocauc)\n",
    "\n",
    "        results_roc_auc.append(np.mean(replicate_roc_auc))\n",
    "        results_training_size.append(train_size)\n",
    "    return(results_roc_auc, results_training_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learning curve\n",
    "\n",
    "We will plot the learning curve, including a moving average (the mean of 5 points). Moving averages can help show trends when data is noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(results_roc_auc_ovr, results_training_size, t):\n",
    "    # Calculate moving average (of last 5 points) with np.convolve\n",
    "    moving_average = np.convolve(results_roc_auc_ovr, np.ones((5,))/5, mode='valid')\n",
    "    # Include an offset to centre mean\n",
    "    x_moving_average = results_training_size[2:-2]\n",
    "\n",
    "    plt.scatter(results_training_size, results_roc_auc_ovr, \n",
    "        label='Accuracy')\n",
    "\n",
    "    plt.plot(x_moving_average, moving_average,\n",
    "        label='Moving average',\n",
    "        color='orange',\n",
    "        linewidth=3)\n",
    "    \n",
    "    if t == 0:\n",
    "        str_title = \"0\"\n",
    "    else:\n",
    "        str_title = f\"0-{t}\"\n",
    "\n",
    "    plt.title(f'Predicting mRS{str_title} at discharge')\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('Test set accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    filename = os.path.join(paths.image_save_path, \n",
    "                            (paths.notebook + paths.model_text + \n",
    "                            f'_learning_curve_mrs{t}.jpg'))\n",
    "    plt.savefig(filename, dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "    return()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the mRS thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for mRS threshold: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb Cell 35\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_bin \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y_bin \u001b[39m=\u001b[39m (y \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m threshold) \u001b[39m*\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb#X51sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m (results_roc_auc, results_training_size) \u001b[39m=\u001b[39m (\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                 train_model_diff_training_set_sizes(n_classes, X, y_bin))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m plot_learning_curve(results_roc_auc, results_training_size, threshold)\n",
      "\u001b[1;32m/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Keep spliting until get one of each category in the training set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_train))\u001b[39m!=\u001b[39mn_classes:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb#X51sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Split data into training and test\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         X, y, test_size \u001b[39m=\u001b[39;49m test_fraction)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb#X51sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# Reduce training set size (use np random choice for random index values)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb#X51sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     selection_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kerry/Documents/GitHub/thrombolysis_clinical_trials_ml_paper/xgb_7_features/015_xgb_all_features_learning_curve.ipynb#X51sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         max_training_size, train_size, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2471\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2467\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m   2469\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[0;32m-> 2471\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\n\u001b[1;32m   2472\u001b[0m     chain\u001b[39m.\u001b[39;49mfrom_iterable(\n\u001b[1;32m   2473\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m arrays\n\u001b[1;32m   2474\u001b[0m     )\n\u001b[1;32m   2475\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2473\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2467\u001b[0m     cv \u001b[39m=\u001b[39m CVClass(test_size\u001b[39m=\u001b[39mn_test, train_size\u001b[39m=\u001b[39mn_train, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m   2469\u001b[0m     train, test \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(cv\u001b[39m.\u001b[39msplit(X\u001b[39m=\u001b[39marrays[\u001b[39m0\u001b[39m], y\u001b[39m=\u001b[39mstratify))\n\u001b[1;32m   2471\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[1;32m   2472\u001b[0m     chain\u001b[39m.\u001b[39mfrom_iterable(\n\u001b[0;32m-> 2473\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m   2474\u001b[0m     )\n\u001b[1;32m   2475\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/sklearn/utils/__init__.py:359\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    354\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSpecifying the columns using strings is only supported for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpandas DataFrames\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m     )\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m    360\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[39mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[39m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/sklearn/utils/__init__.py:201\u001b[0m, in \u001b[0;36m_pandas_indexing\u001b[0;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    196\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m    198\u001b[0m \u001b[39mif\u001b[39;00m key_dtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mint\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m np\u001b[39m.\u001b[39misscalar(key)):\n\u001b[1;32m    199\u001b[0m     \u001b[39m# using take() instead of iloc[] ensures the return value is a \"proper\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[39m# copy that will not raise SettingWithCopyWarning\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mreturn\u001b[39;00m X\u001b[39m.\u001b[39;49mtake(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m    202\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[39m# check whether we should index with loc or iloc\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     indexer \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39miloc \u001b[39mif\u001b[39;00m key_dtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mint\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mloc\n",
      "File \u001b[0;32m~/miniconda3/envs/sam10/lib/python3.10/site-packages/pandas/core/series.py:931\u001b[0m, in \u001b[0;36mSeries.take\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m indices \u001b[39m=\u001b[39m ensure_platform_int(indices)\n\u001b[1;32m    930\u001b[0m new_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mtake(indices)\n\u001b[0;32m--> 931\u001b[0m new_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values\u001b[39m.\u001b[39;49mtake(indices)\n\u001b[1;32m    933\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_values, index\u001b[39m=\u001b[39mnew_index, fastpath\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    934\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for threshold in list_binary_thresholds:\n",
    "\n",
    "    print(f\"Running for mRS threshold: {threshold}\")\n",
    "\n",
    "    y_bin = 0\n",
    "    y_bin = (y <= threshold) * 1\n",
    "\n",
    "    (results_roc_auc, results_training_size) = (\n",
    "                    train_model_diff_training_set_sizes(n_classes, X, y_bin))\n",
    "    plot_learning_curve(results_roc_auc, results_training_size, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we need all, if not more, data to obtain the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "print(f'Time taken: {end_time - start_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('sam10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f85b883bff9a8a9f39576b94acbdf6672b3dc17c35647e7395f81e785740a4b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
